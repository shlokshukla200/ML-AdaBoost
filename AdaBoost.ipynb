{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ElCD6-LBi_k"
      },
      "outputs": [],
      "source": [
        "#Importing the Libraries\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionStump:\n",
        "    def __init__(self):\n",
        "        self.polarity = 1      # The direction of the inequality (1 or -1)\n",
        "        self.feature_idx = None # Index of the feature to split on\n",
        "        self.threshold = None   # Threshold value for the split\n",
        "        self.min_error = float('inf') # The minimum weighted error for this stump\n",
        "\n",
        "    def fit(self, X, y, sample_weights):\n",
        "        \"\"\"\n",
        "        Finds the best feature and threshold to split the data,\n",
        "        based on the minimum weighted error.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Iterate over every feature\n",
        "        for feature_i in range(n_features):\n",
        "            feature_values = X[:, feature_i]\n",
        "            unique_values = np.unique(feature_values)\n",
        "\n",
        "            # Iterate over every unique value as a potential threshold\n",
        "            for threshold in unique_values:\n",
        "                # Try both polarities\n",
        "                for p in [1, -1]:\n",
        "                    predictions = np.ones(n_samples)\n",
        "\n",
        "                    if p == 1:\n",
        "                        # If feature value < threshold, predict -1\n",
        "                        predictions[feature_values < threshold] = -1\n",
        "                    else:\n",
        "                        # If feature value > threshold, predict -1\n",
        "                        predictions[feature_values > threshold] = -1\n",
        "\n",
        "                    # 4. Calculate the weighted error\n",
        "                    weighted_error = np.sum(sample_weights[predictions != y])\n",
        "\n",
        "                    # 5. Check if this is the best stump so far\n",
        "                    if weighted_error < self.min_error:\n",
        "                        self.min_error = weighted_error\n",
        "                        self.polarity = p\n",
        "                        self.threshold = threshold\n",
        "                        self.feature_idx = feature_i\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Makes predictions for a new set of data X.\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        predictions = np.ones(n_samples)\n",
        "        feature_values = X[:, self.feature_idx]\n",
        "\n",
        "        # Classify based on the stored best threshold and polarity\n",
        "        if self.polarity == 1:\n",
        "            predictions[feature_values < self.threshold] = -1\n",
        "        else:\n",
        "            predictions[feature_values > self.threshold] = -1\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "MOxVhXdslmVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#-----------------------------------------------------------------\n",
        "# 2. The AdaBoost Classifier\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "class AdaBoost:\n",
        "    def __init__(self, n_estimators=5):\n",
        "        self.n_estimators = n_estimators # Number of weak learners (stumps) to train\n",
        "        self.clfs = []         # List to store the weak learners\n",
        "        self.alphas = []       # List to store the \"amount of say\" (alpha) for each learner\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Trains the AdaBoost model.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # 1. Initialize sample weights equally\n",
        "        w = np.full(n_samples, (1 / n_samples))\n",
        "\n",
        "        self.clfs = []\n",
        "        self.alphas = []\n",
        "\n",
        "        # 2. Iterate for n_estimators (M in the article)\n",
        "        for _ in range(self.n_estimators):\n",
        "\n",
        "            # 3. Train a weak learner (stump) on the weighted data\n",
        "            clf = DecisionStump()\n",
        "            clf.fit(X, y, sample_weights=w)\n",
        "\n",
        "            # 4. Calculate the weighted error (err) of the stump\n",
        "            predictions = clf.predict(X)\n",
        "\n",
        "            # Sum weights of misclassified samples\n",
        "            # Add a small epsilon to prevent division by zero\n",
        "            err = np.sum(w[predictions != y]) + 1e-10\n",
        "\n",
        "            # 5. Calculate the learner's weight (alpha)\n",
        "            # alpha = 0.5 * log((1 - err) / err)\n",
        "            alpha = 0.5 * np.log((1.0 - err) / err)\n",
        "\n",
        "            # 6. Update the sample weights\n",
        "            # w = w * exp(-alpha * y * predictions)\n",
        "            w *= np.exp(-alpha * y * predictions)\n",
        "\n",
        "            # 7. Normalize the weights (so they sum to 1)\n",
        "            w /= np.sum(w)\n",
        "\n",
        "            # Store the trained learner and its alpha\n",
        "            self.clfs.append(clf)\n",
        "            self.alphas.append(alpha)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Makes a final prediction based on the weighted vote\n",
        "        of all trained weak learners.\n",
        "        \"\"\"\n",
        "        # Get predictions from all weak learners\n",
        "        clf_preds = [alpha * clf.predict(X) for clf, alpha in zip(self.clfs, self.alphas)]\n",
        "\n",
        "        # Sum the weighted predictions\n",
        "        y_pred = np.sum(clf_preds, axis=0)\n",
        "\n",
        "        # The final prediction is the sign of the sum\n",
        "        return np.sign(y_pred)"
      ],
      "metadata": {
        "id": "ac5hz41TmZbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-----------------------------------------------------------------\n",
        "# 3. Example Usage\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(\"Running AdaBoost from scratch...\")\n",
        "\n",
        "    # Generate a binary classification dataset\n",
        "    X, y = make_classification(n_samples=200, n_features=2, n_redundant=0, n_informative=2,\n",
        "                               random_state=1, n_clusters_per_class=1)\n",
        "\n",
        "    # AdaBoost algorithm requires labels to be -1 and 1\n",
        "    y = np.where(y == 0, -1, 1)\n",
        "\n",
        "    # Split into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # --- Train our custom AdaBoost ---\n",
        "    # Using 5 weak learners as in the GFG article's example\n",
        "    adaboost = AdaBoost(n_estimators=5)\n",
        "    adaboost.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = adaboost.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Custom AdaBoost Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "    # --- For comparison: Scikit-learn's AdaBoost ---\n",
        "    from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "    # We use DecisionTreeClassifier(max_depth=1) to mimic our DecisionStump\n",
        "    # 'SAMME' is used for discrete boosting with -1/1 labels\n",
        "    sklearn_ada = AdaBoostClassifier(\n",
        "        n_estimators=5,\n",
        "        algorithm='SAMME',\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    sklearn_ada.fit(X_train, y_train)\n",
        "    sklearn_pred = sklearn_ada.predict(X_test)\n",
        "\n",
        "    sklearn_accuracy = accuracy_score(y_test, sklearn_pred)\n",
        "    print(f\"Scikit-learn AdaBoost Accuracy: {sklearn_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuQlA-slmd4r",
        "outputId": "ffd5b0b4-3d06-44bc-e1ea-46d19193f6a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running AdaBoost from scratch...\n",
            "Custom AdaBoost Accuracy: 0.9000\n",
            "Scikit-learn AdaBoost Accuracy: 0.9000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mmBSxkVKmfwj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}